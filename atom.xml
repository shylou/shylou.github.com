<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[liushy]]></title>
  <subtitle><![CDATA[I'm waiting for you]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://liushy.com/"/>
  <updated>2016-12-24T15:47:47.949Z</updated>
  <id>http://liushy.com/</id>
  
  <author>
    <name><![CDATA[Liushy]]></name>
    
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[来了又走]]></title>
    <link href="http://liushy.com/2016/12/24/goon-2016/"/>
    <id>http://liushy.com/2016/12/24/goon-2016/</id>
    <published>2016-12-24T13:45:06.000Z</published>
    <updated>2016-12-24T13:45:06.000Z</updated>
    <content type="html"><![CDATA[]]></content>
    <summary type="html">
    <![CDATA[]]>
    </summary>
    
      <category term="生活，旅游" scheme="http://liushy.com/tags/%E7%94%9F%E6%B4%BB%EF%BC%8C%E6%97%85%E6%B8%B8/"/>
    
      <category term="Life" scheme="http://liushy.com/categories/Life/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用docker部署hadoop分布式集群]]></title>
    <link href="http://liushy.com/2016/12/21/docker+hadoop/"/>
    <id>http://liushy.com/2016/12/21/docker+hadoop/</id>
    <published>2016-12-21T13:41:34.000Z</published>
    <updated>2016-12-24T14:48:58.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言_"><strong>前言 </strong></h2>
<p>这个部署是去年做的，有点久了，镜像上传在<a href="https://hub.docker.com/r/liushy/ubuntu/tags/" target="_blank" rel="external">dockerhub</a>，想了想，还是写下来，万一哪天有用，可以回顾一下。关于docker的一些内容，后序会写一些，碍于目前生产环境的限制，能做的不会太多，但更多的是熟练docker。hadoop呢，以前读书的时候，断断续续地研究过一段时间，然后就没然后了，事情太多太杂，而自身精力涣散，三天打鱼两天晒网，实际也就没弄明白什么。平时读点博客，看到docker，hadoop等字眼就会不住高潮，内心想说这玩意我玩过啊，然后也就仅限于此了。术业有专攻，如果不是经常接触，搞不久就会忘，若时常回顾一下，等到用时，就不会那么陌生了。   </p>
<h2 id="安装docker"><strong>安装docker</strong></h2>
<p>安装docker的套路网上有教程，目前新的Ubuntu系统都有集成，不过<a href="https://www.oschina.net/translate/installing-docker-on-mac-os-x" target="_blank" rel="external">OSX系统跑docker</a>需要配合虚拟机如Virtualbox等来做，所以mac用户要稍微折腾一下了。本文的环境是在Ubuntu 14下做的，大致命令如下：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> apt-get install apt-transport-https </div><div class="line">$ <span class="built_in">sudo</span> apt-key adv --keyserver hkp://keyserver.ubuntu.com:<span class="number">80</span> --recv-keys <span class="number">36</span>A1D7869245C8950F966E92D8576A8BA88D21E9 </div><div class="line">$ <span class="built_in">sudo</span> bash -c <span class="string">"echo deb https://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list"</span> </div><div class="line">$ <span class="built_in">sudo</span> apt-get update </div><div class="line">$ <span class="built_in">sudo</span> apt-get install lxc-docker</div></pre></td></tr></table></figure>


<p>docker的安装可以参考<a href="http://www.cnblogs.com/xiaoluosun/p/5520510.html" target="_blank" rel="external">这个</a>   </p>
<p>接下来要提到的是docker仓库的概念，熟悉github或使用过svn的都应该知道代码库吧，而我们也可以为docker创建仓库，这个库即是存放docker镜像的场所。比如我们做了个装有JDK的docker镜像，那么就可以将其存到仓库里，每次需要使用带Java的环境时，就可以可以使用该镜像，并在此基础上构建其它镜像。国内比较知名的docker库如<a href="https://github.com/DockerPool/" target="_blank" rel="external">dockerpool</a>，而我此次使用的是<a href="https://hub.docker.com/" target="_blank" rel="external">dockerhub</a>，国内访问剧慢，最好使用vpn加速。建议养成使用仓库的习惯，无论是代码还是docker。怎么使用docker仓库呢，首先需要在dockerhub官网创建一个账户，创建自己的仓库，然后在你的生产环境登陆<a href="https://hub.docker.com/" target="_blank" rel="external">dockerhub</a>，即可从<a href="https://hub.docker.com/" target="_blank" rel="external">dockerhub</a>上pull（拉取）镜像，当然也可以将本地的镜像push（上传）到仓库。大致流程可<a href="http://geek.csdn.net/news/detail/35121" target="_blank" rel="external">参考此文</a>。  </p>
<p>登录到DockerHub：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ docker login --username=账户 --email=邮箱@xxx.com</div><div class="line">PassWord:</div><div class="line">WARNING: login credentials saved <span class="keyword">in</span> /home/hadoop/.docker/config.json</div><div class="line">Login Succeeded</div></pre></td></tr></table></figure>

<p>搞定仓库设置后，那么就可以从docker仓库中获取Ubuntu镜像了：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker pull ubuntu:<span class="number">14.04</span></div></pre></td></tr></table></figure>

<p>使用<code>docker images</code>可以查看本地的所有镜像：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop:~$ docker images</div><div class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</div><div class="line">liushy/ubuntu       hadoop              <span class="number">358</span>d6e31872e        <span class="number">13</span> months ago       <span class="number">1.276</span> GB</div><div class="line">liushy/ubuntu       java                <span class="number">50</span>d62690f3f0        <span class="number">13</span> months ago       <span class="number">752.9</span> MB</div><div class="line">ubuntu              <span class="number">14.04</span>               e9ae3c220b23        <span class="number">13</span> months ago       <span class="number">187.9</span> MB</div></pre></td></tr></table></figure>


<p>显示的内容包括：   </p>
<ul>
<li>REPOSITORY：仓库名，例如liushy/ubuntu和ubuntu</li>
<li>TAG：标记,例如hadoop</li>
<li>IMAGE ID：镜像ID号，这是唯一的</li>
<li>CREATED：创建时间</li>
<li>SIZE：镜像大小   </li>
</ul>
<p>那么，除此之外，还需要了解docker的一些常用操作。   </p>
<h2 id="启动Docker容器_"><strong>启动Docker容器 </strong></h2>
<p>启动docker容器，就可以构建部署hadoop集群了。使用如下命令启动docker容器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -ti ubuntu</div></pre></td></tr></table></figure>

<p>docker run -ti ubuntu命令中没有指定执行程序，Docker默认执行/bin/bash。如下面这条命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop:~$  docker run liushy/ubuntu:java /bin/<span class="built_in">echo</span> <span class="string">'Hello world'</span></div><div class="line">Hello world</div></pre></td></tr></table></figure>


<p>即是启动标记为java的镜像，运行bash打印“Hello world”。<br>接下来我们在Ubuntu基础镜像的上，安装Java，执行如下命令：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">50</span>d62690f3f0:~<span class="comment">#sudo apt-get install software-properties-common python-software-properties</span></div><div class="line">root@<span class="number">50</span>d62690f3f0:~<span class="comment">#sudo add-apt-repository ppa:webupd8team/java</span></div><div class="line">root@<span class="number">50</span>d62690f3f0:~<span class="comment">#sudo apt-get update</span></div><div class="line">root@<span class="number">50</span>d62690f3f0:~<span class="comment">#sudo apt-get install oracle-java7-installer</span></div></pre></td></tr></table></figure>


<p>安装结束后，可以将该镜像保存，以备以后使用：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">50</span>d62690f3f0:~<span class="comment"># exit</span></div><div class="line">docker commit -m <span class="string">"java image"</span> <span class="number">50</span>d62690f3f0  liushy/ubuntu:java</div></pre></td></tr></table></figure>


<p><code>-m</code>后面指定提交说明，<code>50d62690f3f0</code>是容器ID（也可以通过<code>docker ps</code>查询运行的容器），<code>liushy/ubuntu</code>是仓库，<code>:java</code>是标记。</p>
<h2 id="构建Hadoop镜像"><strong>构建Hadoop镜像</strong></h2>
<p>首先启动Java容器的镜像，下载Hadoop，输入如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop:~$ docker run -ti liushy/ubuntu:java</div><div class="line">root@<span class="number">7</span>cf73e2147ec:/<span class="comment"># </span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:<span class="built_in">cd</span> ~</div><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># mkdir soft</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># cd soft/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft<span class="comment"># mkdir apache</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft<span class="comment"># cd apache/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache<span class="comment"># mkdir hadoop</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache<span class="comment"># cd hadoop/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop<span class="comment"># wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop<span class="comment"># tar xvzf hadoop-2.6.0.tar.gz</span></div></pre></td></tr></table></figure>


<p>本次安装的是hdoop-2.6.0版的   </p>
<h3 id="配置环境">配置环境</h3>
<p>还需要配置环境变量，在~/.bashrc添加java和hadoop文件路径：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">7</span>cf73e2147ec:vi ~/.bashrc</div><div class="line"><span class="keyword">export</span> JAVA_HOME=/usr/lib/jvm/java-<span class="number">7</span>-oracle</div><div class="line"><span class="keyword">export</span> HADOOP_HOME=/root/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span></div><div class="line"><span class="keyword">export</span> HADOOP_CONFIG_HOME=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</div><div class="line"><span class="keyword">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</div><div class="line"><span class="keyword">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</div></pre></td></tr></table></figure>


<p>最后，一定记得要<code>source ~/.bashrc</code>使配置生效。</p>
<h3 id="配置Hadoop">配置Hadoop</h3>
<p>部署分布式的hadoop，主要分为两大角色：Master和Slave。从HDFS的角度，由若干个NameNode和DataNode组成（在分布式文件系统中，NameNode管理文件系统的命名空间，DataNode管理存储的数据）；从MapReduce的角度，将主机划分JobTracker 和TaskTracker(主节点的Job分配多个Task给从节点执行)。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务<a href="http://blog.chinaunix.net/uid-25266990-id-3900239.html" target="_blank" rel="external">[参考]</a>。   </p>
<p>部署hadoop的各个节点，需要修改hadoop的配置文件，包括core-site.xml、hdfs-site.xml、mapred-site.xml这三个文件。在此之前，建议新建如下目录:</p>
<ul>
<li>tmp：作为Hadoop的临时目录</li>
<li>namenode：作为NameNode的存放目录</li>
<li>datanode：作为DataNode的存放目录</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># cd $HADOOP_HOME/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># mkdir tmp</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># cd tmp/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/tmp<span class="comment"># cd ../</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># mkdir namenode</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># cd namenode/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/namenode<span class="comment"># cd ../</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># mkdir datanode</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span><span class="comment"># cd datanode/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec::~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/datanode<span class="comment"># cd $HADOOP_CONFIG_HOME/</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/etc/hadoop<span class="comment"># cp mapred-site.xml.template mapred-site.xml</span></div></pre></td></tr></table></figure>


<p>接下来就是对上述几个文件进行配置：     </p>
<p><strong>core-site.xml配置：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line">            <span class="tag">&lt;<span class="title">description</span>&gt;</span>xxxxxx.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></div></pre></td></tr></table></figure>


<ul>
<li>hadoop.tmp.dir：配置为/root/soft/apache/hadoop/hadoop-2.6.0/tmp为此前创建的临时目录。</li>
<li>fs.default.name：配置为hdfs://master:9000，指向Master节点  </li>
</ul>
<p><strong>hdfs-site.xml配置：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">description</span>&gt;</span>xxxxx.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/namenode<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/datanode<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></div></pre></td></tr></table></figure>


<ul>
<li>dfs.replication：配置为2。指集群为一个Master节点和两个Slave节点。</li>
<li>dfs.namenode.name.dir：配置为此前创建的NameNode目录</li>
<li>dfs.datanode.data.dir：配置为此前创建的NaDataNode目录   </li>
</ul>
<p><strong>mapred-site.xml配置：</strong>   </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>master:9001<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">description</span>&gt;</span>xxxxxx.<span class="tag">&lt;/<span class="title">description</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></div></pre></td></tr></table></figure>


<ul>
<li>mapred.job.tracker：配置jobTracker在master节点。 </li>
</ul>
<p>除此之外，还要配置conf/hadoop-env.sh文件,修改为你的jdk的安装位置:   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">export</span> JAVA_HOME=/usr/lib/jvm/java-<span class="number">7</span>-oracle</div></pre></td></tr></table></figure>


<p>还要格式化Namenode:<code>hadoop namenode -format</code></p>
<h3 id="节点SSH互访">节点SSH互访</h3>
<p>Hadoop启动以后，Namenode是通过SSH来启动和停止各个Datanode上的守护进程的，要求在节点之间执行指令的时候是不需要输入密码，故我们要配置SSH运用无密码公钥认证的形式。   </p>
<p>首先，安装SSH   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># sudo apt-get install ssh</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># ssh localhost</span></div></pre></td></tr></table></figure>

<p>利用<code>ssh localhost</code>测试一下是否设置好无口令登陆，如果没有设置好，系统将要求你输入密码，通过下面的设置可以实现无口令登陆：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa</span></div><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></div></pre></td></tr></table></figure>


<p>最后，保存一份镜像:   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@<span class="number">7</span>cf73e2147ec:~<span class="comment"># exit</span></div><div class="line">hadoop@hadoop:~$ docker commit -m <span class="string">"hadoop image"</span> <span class="number">358</span>d6e31872e liushy/ubuntu:hadoop</div></pre></td></tr></table></figure>

<h2 id="启动Hadoop集群"><strong>启动Hadoop集群</strong></h2>
<p>接下来就是启动hadoop集群了，本次部署的hadoop方案是，一个Master节点，两个Slave节点： 　　</p>
<font color="blue" size="4">#部署方案:</font>    

<table>
<thead>
<tr>
<th style="text-align:center">节点</th>
<th style="text-align:center">IP</th>
<th style="text-align:center">Hadoop任务</th>
<th style="text-align:center">Docker启动命令</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Master</td>
<td style="text-align:center">172.17.0.2</td>
<td style="text-align:center">Namenode,jobTracker</td>
<td style="text-align:center">docker run -ti -h master liushy/ubuntu:hadoop</td>
</tr>
<tr>
<td style="text-align:center">Slave1</td>
<td style="text-align:center">172.17.0.3</td>
<td style="text-align:center">Datanode,taskTracker</td>
<td style="text-align:center">docker run -ti -h Slave1 liushy/ubuntu:hadoop</td>
</tr>
<tr>
<td style="text-align:center">Slave2</td>
<td style="text-align:center">172.17.0.4</td>
<td style="text-align:center">Datanode,taskTracker</td>
<td style="text-align:center">docker run -ti -h Slave2 liushy/ubuntu:hadoop</td>
</tr>
</tbody>
</table>
<p>容器启动成功后，docker会为它们自动分配ip地址，是同一网段相互之间能够ping通。当然也可以修改ip，方法请自行百度。   </p>
<h3 id="修改hosts">修改hosts</h3>
<p>接下来修改各节点的hosts文件<code>vi /etc/hosts</code>，添加各节点的hostname和ip：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">172.17</span>.<span class="number">0.2</span>        master</div><div class="line"><span class="number">172.17</span>.<span class="number">0.3</span>        slave1</div><div class="line"><span class="number">172.17</span>.<span class="number">0.4</span>        slave2</div></pre></td></tr></table></figure>


<h3 id="修改slaves">修改slaves</h3>
<p>除此之外，还要在master节点上，配置slaves文件，该文件需要填写slave节点的hostname:   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@master:~<span class="comment"># cd $HADOOP_CONFIG_HOME/</span></div><div class="line">root@master:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/etc/hadoop<span class="comment"># vi slaves</span></div><div class="line">slave1</div><div class="line">slave2</div></pre></td></tr></table></figure>


<h3 id="启动Hadoop">启动Hadoop</h3>
<p>在Master节点的hadoop目录下执行<code>start-all.sh</code>，启动Hadoop集群:   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">root@master:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/etc/hadoop<span class="comment"># start-all.sh </span></div><div class="line">This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh</div><div class="line">starting secondarynamenode, logging to /root/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/logs/hadoop-root-secondarynamenode-master.out</div><div class="line">starting yarn daemons</div><div class="line">...</div><div class="line"><span class="number">0.0</span>.<span class="number">0.0</span>:starting resourcemanager, logging to /root/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/logs/yarn--resourcemanager-master.out</div><div class="line">slave2: starting nodemanager, logging to /root/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/logs/yarn-root-nodemanager-slave2.out</div><div class="line">slave1: starting nodemanager, logging to /root/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/logs/yarn-root-nodemanager-slave1.out</div></pre></td></tr></table></figure>

<p>打印上述日志，说明集群运行成功，碍于生产环境，运行较慢，可以多等等。使用jps命令可以查看各节点运行的进程。 </p>
<p><strong>master节点：</strong>    </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root@master:~/soft/apache/hadoop/hadoop-<span class="number">2.6</span>.<span class="number">0</span>/etc/hadoop<span class="comment"># jps</span></div><div class="line"><span class="number">492</span> Jps</div><div class="line"><span class="number">429</span> ResourceManager</div><div class="line"><span class="number">295</span> SecondaryNameNode</div><div class="line"><span class="number">124</span> NameNode</div></pre></td></tr></table></figure>



<p><strong>slave1节点：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@slave1:/<span class="comment"># jps</span></div><div class="line"><span class="number">52</span> DataNode</div><div class="line"><span class="number">147</span> NodeManager</div><div class="line"><span class="number">240</span> Jps</div></pre></td></tr></table></figure>



<p><strong>slave2节点：</strong>   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@slave1:/<span class="comment"># jps</span></div><div class="line"><span class="number">50</span> DataNode</div><div class="line"><span class="number">121</span> NodeManager</div><div class="line"><span class="number">250</span> Jps</div></pre></td></tr></table></figure>


<p>通过web访问hadoop运行情况：<br><img src="/img/docker-hadoop.png" alt="hadoop-web"><br>针对hadoop的架构和操作此文就不深入阐述了。</p>
<h2 id="结语"><strong>结语</strong></h2>
<p>搭建环境是一件费时耗力，低效率的事情，搞不好就各种排错，甚至重来，这也是很多人不想接触环境的原因。如何做到少出错，快速定位问题，我想这也是很多正在搞环境的开发人员所渴望的一种工作状态。其实，没有什么捷径可走，还是要多积累，但有些事情必须得长期坚持，那就是学习操作系统，内存管理，网络等相关知识。很多疑难杂症多因环境变量的配置，硬盘故障，网络不通等引起，而我们往往因为对这些知识不熟悉，而手足无措。    </p>
<p>回到本文的主题，docker是这两年势头正旺的开源产品，基于lxc（linux container），使得应用部署更加轻量。如何将其运用于开发环境中，很多互联网公司都在做。作为开发人员来说，掌握容器也是一门专业利器，基于容器结合应用可以实现大规模的业务部署，比如本文的hadoop。当然相比虚拟机，docker也有缺陷，比如隔离性差，所以如何运用docker来保障处理业务的安全稳定，是开发人员需要通过实践来解决的问题。   </p>
<p><strong>参考网站</strong><br><a href="http://blog.chinaunix.net/uid-25266990-id-3900239.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-25266990-id-3900239.html</a><br><a href="http://blog.csdn.net/u011692203/article/details/46898293" target="_blank" rel="external">http://blog.csdn.net/u011692203/article/details/46898293</a><br><a href="http://www.cnblogs.com/xiaoluosun/p/5520510.html" target="_blank" rel="external">http://www.cnblogs.com/xiaoluosun/p/5520510.html</a><br><a href="http://tashan10.com/yong-dockerda-jian-hadoopwei-fen-bu-shi-ji-qun/#" target="_blank" rel="external">http://tashan10.com/yong-dockerda-jian-hadoopwei-fen-bu-shi-ji-qun/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言_"><strong>前言 </strong></h2>
<p>这个部署是去年做的，有点久了，镜像上传在<a href="https://hub.docker.com/r/liushy/ubuntu/tags/" target="_blank" rel="ex]]>
    </summary>
    
      <category term="docker" scheme="http://liushy.com/tags/docker/"/>
    
      <category term="Hadoop" scheme="http://liushy.com/tags/Hadoop/"/>
    
      <category term="Linux" scheme="http://liushy.com/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux内存占用]]></title>
    <link href="http://liushy.com/2016/11/27/Linux-mem/"/>
    <id>http://liushy.com/2016/11/27/Linux-mem/</id>
    <published>2016-11-27T02:39:51.000Z</published>
    <updated>2016-11-27T03:39:56.000Z</updated>
    <content type="html"><![CDATA[<p>记录一下几个常用的查看Linux内存和硬盘的命令，以备查看使用：   </p>
<h3 id="top">top</h3>
<p>top命令是Linux下常用的性能分析工具，能够实时显示系统中进程的资源占用状况，类似于Windows的任务管理器可以直接使用top命令后，查看%MEM的内容。可以指定进程或者用户查看，如查看oracle用户的进程内存使用情况的话可以使用如下的命令：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ top -u oracle</div></pre></td></tr></table></figure>

<p>显示内容解释：</p>
<blockquote>
<p>PID：进程的ID<br>USER：进程所有者<br>PR：进程的优先级别，越小越优先被执行<br>NInice：值<br>VIRT：进程占用的虚拟内存<br>RES：进程占用的物理内存<br>SHR：进程使用的共享内存<br>S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示该进程优先值为负数<br>%CPU：进程占用CPU的使用率<br>%MEM：进程使用的物理内存和总内存的百分比<br>TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。<br>COMMAND：进程启动命令名称   </p>
</blockquote>
<p>常用的命令：</p>
<blockquote>
<p>P：按%CPU使用率排行<br>T：按MITE+排行<br>M：按%MEM排行   </p>
</blockquote>
<h3 id="pmap">pmap</h3>
<p>可以根据进程查看进程相关信息占用的内存情况，(进程号可以通过ps查看)如下所示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ pmap <span class="operator">-d</span> <span class="number">14596</span></div></pre></td></tr></table></figure>


<h3 id="ps">ps</h3>
<p>ps 的参数众多, 在此仅列出几个常用的参数：   </p>
<blockquote>
<p>-A 列出所有的行程<br>-w 显示加宽可以显示较多的资讯<br>-au 显示较详细的资讯<br>-aux 显示所有包含其他使用者的行程   </p>
</blockquote>
<p>au(x) 输出格式 :<br>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND   </p>
<blockquote>
<p>USER: 行程拥有者<br>PID: pid<br>%CPU: 占用的 CPU 使用率<br>%MEM: 占用的记忆体使用率<br>VSZ: 占用的虚拟记忆体大小<br>RSS: 占用的记忆体大小<br>TTY: 终端的次要装置号码 (minor device number of tty)<br>STAT: 该行程的状态:<br>D: 不可中断的静止<br>R: 正在执行中<br>S: 静止状态<br>T: 暂停执行<br>Z: 不存在但暂时无法消除<br>W: 没有足够的记忆体分页可分配<br>&lt;: 高优先序的行程<br>N: 低优先序的行程<br>L: 有记忆体分页分配并锁在记忆体内<br>START: 行程开始时间<br>TIME: 执行的时间<br>COMMAND:所执行的指令   </p>
</blockquote>
<h3 id="du">du</h3>
<p>du命令主要是用来查看硬盘使用情况的：  </p>
<blockquote>
<p>  du -sh : 查看当前目录总共占的容量。而不单独列出各子项占用的容量<br>du -lh —max-depth=1 : 查看当前目录下一级子文件和子目录占用的磁盘容量。   </p>
</blockquote>
<h3 id="free">free</h3>
<p>free是最常用的查看内存占用的命令，有以下参数：   </p>
<blockquote>
<p>-b：以Byte为单位显示内存使用情况；<br> -k：以KB为单位显示内存使用情况；<br>-m：以MB为单位显示内存使用情况；<br>-o：不显示缓冲区调节列；<br>-s&lt;间隔秒数&gt;：持续观察内存使用状况；<br>-t：显示内存总和列；<br>-V：显示版本信息。<br>举例说明：  </p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">free -m </div><div class="line">            total  used  free shared  buffers cached </div><div class="line">Mem:        <span class="number">2016</span>   <span class="number">1973</span>  <span class="number">42</span>     <span class="number">0</span>     <span class="number">163</span>    <span class="number">1497</span> </div><div class="line">-/+ buffers/cache: <span class="number">312</span>   <span class="number">1703</span> </div><div class="line">Swap:       <span class="number">4094</span>    <span class="number">0</span>    <span class="number">4094</span></div></pre></td></tr></table></figure>


<p>输出的第一行：   </p>
<blockquote>
<p>total：内存总数；<br>used：已经使用的内存数；<br>free：空闲的内存数；<br>shared：当前已经废弃不用；<br>buffers Buffer：缓存内存数；<br>cached Page：缓存内存数。   </p>
</blockquote>
<p>第二行（-/+ buffers/cache）：   </p>
<blockquote>
<p>(-buffers/cache) used内存数：第一行Mem行中的 used – buffers – cached<br>(+buffers/cache) free内存数: 第一行Mem行中的 free + buffers + cached   </p>
</blockquote>
<p>第三行是交换区的使用情况，那么什么时候会用到交换区的内存呢，即当可用内存少于额定值的时候，就会进行交换。可以通过<code>cat /proc/meminfo</code>来查看内存额定值：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">hadoop@hadoop:~$ cat /proc/meminfo </div><div class="line">MemTotal:        <span class="number">1017576</span> kB</div><div class="line">MemFree:          <span class="number">206012</span> kB</div><div class="line">MemAvailable:     <span class="number">479832</span> kB</div><div class="line">Buffers:           <span class="number">47344</span> kB</div><div class="line">Cached:           <span class="number">333836</span> kB</div><div class="line">SwapCached:            <span class="number">0</span> kB</div><div class="line">Active:           <span class="number">455128</span> kB</div><div class="line">Inactive:         <span class="number">271084</span> kB</div><div class="line">Active(anon):     <span class="number">345856</span> kB</div><div class="line">Inactive(anon):     <span class="number">4140</span> kB</div><div class="line">Active(file):     <span class="number">109272</span> kB</div><div class="line">Inactive(file):   <span class="number">266944</span> kB</div><div class="line">Unevictable:          <span class="number">32</span> kB</div><div class="line">Mlocked:              <span class="number">32</span> kB</div><div class="line">SwapTotal:       <span class="number">1047548</span> kB</div><div class="line">SwapFree:        <span class="number">1047548</span> kB</div><div class="line">Dirty:               <span class="number">588</span> kB</div><div class="line">Writeback:             <span class="number">0</span> kB</div><div class="line">AnonPages:        <span class="number">345060</span> kB</div><div class="line">Mapped:           <span class="number">174264</span> kB</div><div class="line">Shmem:              <span class="number">4968</span> kB</div><div class="line">Slab:              <span class="number">38328</span> kB</div><div class="line">SReclaimable:      <span class="number">20452</span> kB</div><div class="line">SUnreclaim:        <span class="number">17876</span> kB</div><div class="line">KernelStack:        <span class="number">5312</span> kB</div><div class="line">PageTables:        <span class="number">23708</span> kB</div><div class="line">NFS_Unstable:          <span class="number">0</span> kB</div><div class="line">Bounce:                <span class="number">0</span> kB</div><div class="line">WritebackTmp:          <span class="number">0</span> kB</div><div class="line">CommitLimit:     <span class="number">1556336</span> kB</div><div class="line">Committed_AS:    <span class="number">2404256</span> kB</div><div class="line">VmallocTotal:   <span class="number">34359738367</span> kB</div><div class="line">VmallocUsed:       <span class="number">23324</span> kB</div><div class="line">VmallocChunk:   <span class="number">34359709832</span> kB</div><div class="line">HardwareCorrupted:     <span class="number">0</span> kB</div><div class="line">AnonHugePages:     <span class="number">65536</span> kB</div><div class="line">HugePages_Total:       <span class="number">0</span></div><div class="line">HugePages_Free:        <span class="number">0</span></div><div class="line">HugePages_Rsvd:        <span class="number">0</span></div><div class="line">HugePages_Surp:        <span class="number">0</span></div><div class="line">Hugepagesize:       <span class="number">2048</span> kB</div></pre></td></tr></table></figure>

]]></content>
    <summary type="html">
    <![CDATA[<p>记录一下几个常用的查看Linux内存和硬盘的命令，以备查看使用：   </p>
<h3 id="top">top</h3>
<p>top命令是Linux下常用的性能分析工具，能够实时显示系统中进程的资源占用状况，类似于Windows的任务管理器可以直接使用top命令后，查看]]>
    </summary>
    
      <category term="内存" scheme="http://liushy.com/tags/%E5%86%85%E5%AD%98/"/>
    
      <category term="Linux" scheme="http://liushy.com/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[OVS配置命令]]></title>
    <link href="http://liushy.com/2016/11/26/OVS-CLI/"/>
    <id>http://liushy.com/2016/11/26/OVS-CLI/</id>
    <published>2016-11-26T10:12:53.000Z</published>
    <updated>2016-11-26T12:23:29.000Z</updated>
    <content type="html"><![CDATA[<p>在ovs结构中，如果网络拓扑是vxlan或gre，则有两个bridge，分别是br-int和br-tun，br-int叫集成网桥，用于连接起上方的各个设备（包括vm、dhcp-agent、l3-agent），br-tun叫隧道网桥，隧道既可以是gre，也可以是vxlan，br-tun负责在原始报文中加入gre或vxlan报文头。相当于软件实现了vtep设备(对于vxlan而言)<br>以下是本人使用和收集的OVS相关配置命令：       </p>
<p>建立 VXLAN Network ID (VNI) 和指定的 OpenFlow port number, eg: VNI=5566, OF_PORT=9   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> interface vxlan <span class="built_in">type</span>=vxlan option:remote_ip=x.x.x.x option:key=<span class="number">5566</span> ofport_request=<span class="number">9</span></div></pre></td></tr></table></figure>



<p>VNI flow by flow</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> interface vxlan <span class="built_in">type</span>=vxlan option:remote_ip=<span class="number">140.113</span>.<span class="number">215.200</span> option:key=flow ofport_request=<span class="number">9</span></div></pre></td></tr></table></figure>



<p>设置 VXLAN tunnel id   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ovs-ofctl add-flow ovs-br <span class="keyword">in</span>_port=<span class="number">1</span>,actions=<span class="keyword">set</span>_field:<span class="number">5566</span>-&gt;tun_id,output:<span class="number">2</span></div><div class="line">ovs-ofctl add-flow s1 <span class="keyword">in</span>_port=<span class="number">2</span>,tun_id=<span class="number">5566</span>,actions=output:<span class="number">1</span></div></pre></td></tr></table></figure>



<p>查询out-of-band $ in-band   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl get controller ovs-br connection-mode</div></pre></td></tr></table></figure>



<p>Out-of-band   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> controller ovs-br connection-mode=out-of-band</div></pre></td></tr></table></figure>



<p>In-band (default)   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> controller ovs-br connection-mode=<span class="keyword">in</span>-band</div></pre></td></tr></table></figure>


<p>移除 hidden flow   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> bridge br0 other-config:disable-in-band=<span class="literal">true</span></div></pre></td></tr></table></figure>


<p>设置 GRE tunnel   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs−vsctl add−port ovs-br ovs-gre -- <span class="keyword">set</span> interface ovs-gre <span class="built_in">type</span>=gre options:remote_ip=<span class="number">1.2</span>.<span class="number">3.4</span></div></pre></td></tr></table></figure>


<p>设置 Vlan trunk   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl add-port ovs-br eth0 trunk=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span></div></pre></td></tr></table></figure>


<p>设置已 add 的 port 为 access port, vlan id 9</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ovs-vsctl <span class="keyword">set</span> port eth0 tag=<span class="number">9</span></div></pre></td></tr></table></figure>


<p>网络工程是一门大学问，虽然本科四年里偶有涉及，硕士阶段专攻了一年多的路由交换原理，到目前开始从事网络安全开发工作，我也不敢说自己精通网络，深知自身仍然还有很多要学的地方，而对于网络知识的学习，我觉得最好的方式莫过于搞一套网络设备来慢慢折腾，模拟一个个应用场景，敲一下各种配置命令，能够一步步照着做了，想弄明白网络就不是难事，总的来说，急不得，在了解理论的基础上，动手实践理解会更深刻，还有一点就是多做记录，正所谓好记性不如烂笔头，我推荐做一个自己的博客或者使用笔记软件，记录自己平时的实践和理论学习。<br>参考网站：<br><a href="http://blog.csdn.net/beginning1126/article/details/41172365" title=" http://blog.csdn.net/beginning1126/article/details/41172365" target="_blank" rel="external"> http://blog.csdn.net/beginning1126/article/details/41172365</a><br><a href="http://blog.csdn.net/tantexian/article/details/46707175" title="http://blog.csdn.net/tantexian/article/details/46707175" target="_blank" rel="external">http://blog.csdn.net/tantexian/article/details/46707175</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在ovs结构中，如果网络拓扑是vxlan或gre，则有两个bridge，分别是br-int和br-tun，br-int叫集成网桥，用于连接起上方的各个设备（包括vm、dhcp-agent、l3-agent），br-tun叫隧道网桥，隧道既可以是gre，也可以是vxlan，b]]>
    </summary>
    
      <category term="OVS" scheme="http://liushy.com/tags/OVS/"/>
    
      <category term="VXLAN" scheme="http://liushy.com/tags/VXLAN/"/>
    
      <category term="GRE" scheme="http://liushy.com/tags/GRE/"/>
    
      <category term="SDN" scheme="http://liushy.com/categories/SDN/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[随便聊聊]]></title>
    <link href="http://liushy.com/2015/11/12/talk--life/"/>
    <id>http://liushy.com/2015/11/12/talk--life/</id>
    <published>2015-11-12T12:34:44.000Z</published>
    <updated>2016-11-26T10:43:48.000Z</updated>
    <content type="html"><![CDATA[<h2 id="唠唠">唠唠</h2>
<p>快到年终了，该来个年终总结什么的。。。咳咳。。好久没发文了，本博创建伊始主要是为了记录自己平时的技术心得，距离上次发表文章超过半年了，很恐怖啊，其实这段时间也看了不少文章，还是有不少感悟。细细想来还是自己太懒了，当然中途有部分时间在写自己的小论文，找工作balala，很累，以至于忘了搭理自己这个博客了。现在忙的事情都先告一段落了，还好，今天得空了，想了想，还是得对自己这几个月梳理梳理，由于本文与技术无关，所以新建了一个专题 [<em>Life</em>] 打算以后写不出技术文章的时候，就在这里说说自己的生活感悟。</p>
<h2 id="聊聊">聊聊</h2>
<p>其实这半年是真的很累，距离上一次发文不久，我就参加了全国的大学生SDN应用创新大赛，准备了一个多月，很无奈，没获奖，只得到了一个参赛证明，我们属于研究生组的。比较喜剧的是，本校有一个本科生参赛组在提交作品前，还请我们去给人家指导，结果人家最后进入了决赛，还拿了奖。。。汗。。。不得不说师弟们很厉害哈！师兄没起到带头作用，丢人了。。。<br>然后是上面提到的找工作，说实话，我觉得找工作这事儿特诡异，怎么说呢？面试ZX是一个很偶然的时候，当时ZX来重庆招蓝剑，说实话，蓝剑是什么东东，当时的我一概不知，所以也就没管，恰好这时，本实验室一哥们去面了，完了特兴奋地给我打电话喊我去面试。当时我还没反应过来怎么回事，还怀疑这哥们是不是受刺激了，然后和实验室另一基友去面了。。面试过程就不扒了，此处省略N个字。。。现在算半只脚踏进了zx里了，前段时间，部门来电话了，说是做SDN，主要方向是云安全的，这下好了，心里算踏实了，一直以来我都是比较关注云安全这一块的，然后还要结合SDN来做，哈哈，这不是为我量身打造的吗~心里挺满意的哈，不过也得自己有那个能力，以后嘛~~好好干涩！ 送上一张蓝剑夏令营，在深圳拍的<br><img src="/img/lanjian.JPG" alt="lanjian"></p>
<h2 id="玩玩">玩玩</h2>
<p>前段时间出去玩了，好久没这么嗨了，十月底出去的，同室友一起去了龚滩古镇，西昌和泸沽湖。玩的挺累的，不过景色好心情也好，还有好客的当地人，算是不错的一趟旅行。第一站来到了龚滩古镇，位于重庆与贵州的交界处——酉阳县，古镇依江而建，镇对面是拔地而起的万丈高崖，江面绿波荡漾，逛了一圈花了将近一下午的时间，很是羡慕当地人，面朝江面，４Ｍ带宽的生活，有些生活细节让我莫名的感动，看到有一家人坐在电视机前吃饭，小孩子一边扒饭一边盯着电视看动画片，爸妈都在桌前吃饭聊着什么，孩子他妈时不时提醒小朋友多吃点饭别老看电视，这个场景让我想起了小时候，内心忍不住有些感慨。当夜我们就住进了一处江边的客栈，超级便宜，每人４０元，热水ｗｉｆｉ电视该有的都有，晚上坐在江边的馆子里吃了些当地的特色，味道还行，略贵－——－！甩张图给你们看一下：   　　　</p>
<p><img src="/img/gongtan.JPG" alt="gongtan"><br>第二站来到了西昌，这个日照充足的地方很有意思，下了火车我还以为来到了西北一样，整个地区很平不像重庆山多，灰尘蛮大的，各种工程车行驶而过。人生地不熟的，没瞎逛我们就直奔传说中的琼海了，琼海其实就是一个大湖，但时不时来点浪哈，一股咸盐味。海边修的公园，不远处是一块湿地，当天逛完了琼海我们就租了自行车在湿地里逛了一圈，路面很宽干净，在加上西昌的阳光，非常不错的骑行。晚上吃了西昌当地的特色——醉虾，活虾就那么放在一堆作料里闷，吃在嘴里还在跳，怪怪的。甩一张琼海的<br><img src="/img/xichang.JPG" alt="qionghai"><br>第三站我们来到了美丽的女儿国——泸沽湖，第一次认识泸沽湖，是在[转山]这本书里，网上搜索了泸沽湖，顿时被图片里的她惊艳了，一直想去，但被各种原因搁浅了。这个湖太大了，连接了四川和云南两省，绕湖一圈七十多公里，而且还分了四川泸沽湖和云南泸沽湖。下车我们就先去了草海，这地方是泸沽湖旁的一块湿地，据说以前走婚的时候，男方就是划着船儿过草海来到摩梭人家的，晚上去次日早上回，当地人为了安全，每家每户出钱出力修了一条走婚桥。草海真是纯净无污染，收图<br><img src="/img/caohai.JPG" alt="caohai"><br>第一天夜里住在了里格的一家青年旅社，真心便宜，这边的客栈房间里几乎都提供了电热毯，所以晚上睡着不会太冷。第二天早上天没亮就出门了，哈哈，看日出。船家将我们送到一个视野最好的湖滩上等日出，日出难得啊，可是冷啊～雾好大啊～但还好，等到了<br><img src="/img/richu.JPG" alt="richu"><br>最后一天就是绕湖观光，走得很累，但景色确实不错，纯净自然，阳光充足，比雾都重庆好太多，感觉自己就是来这洗肺的，看看这纯净的湖面<br><img src="/img/luguhu.JPG" alt="luguhu">　　　</p>
<h2 id="哈哈">哈哈</h2>
<p>最后以此标题做个总结吧，这一年就这样结束了，没什么大事儿，感觉就是这么细水长流般渡过了，玩也玩了，该完成的项目也完成了。这一年收获蛮大的，事实上相比一年前，懂得东西更多了，更深入了，还有了一定的成果，可喜可贺，工作定了还算比较顺利，结识了一些搞技术的朋友，知识面更开阔了，也发现了自身诸多的不足。开心的是出去旅行一圈的愿望实现了，特别是去了我梦里的女儿国哈！新年伊始还有很多事要做，毕业论文什么的，为上班做好准备等等～好好干吧，孩子！新年快乐，么么哒～</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="唠唠">唠唠</h2>
<p>快到年终了，该来个年终总结什么的。。。咳咳。。好久没发文了，本博创建伊始主要是为了记录自己平时的技术心得，距离上次发表文章超过半年了，很恐怖啊，其实这段时间也看了不少文章，还是有不少感悟。细细想来还是自己太懒了，当然中途有部分时间在写]]>
    </summary>
    
      <category term="生活，旅游" scheme="http://liushy.com/tags/%E7%94%9F%E6%B4%BB%EF%BC%8C%E6%97%85%E6%B8%B8/"/>
    
      <category term="Life" scheme="http://liushy.com/categories/Life/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Floodlight中模块对Packet_in报文的处理]]></title>
    <link href="http://liushy.com/2015/04/07/Floodlight-packetin/"/>
    <id>http://liushy.com/2015/04/07/Floodlight-packetin/</id>
    <published>2015-04-07T14:03:26.000Z</published>
    <updated>2016-11-26T14:12:57.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>看源码最痛苦的是知其然而不知其所以然，当然也有人建议我不要在意这些细节~~，可是看不懂浑身不舒服啊亲，我也希望自己有高超的写代码能力，事实是还需努力。接触Floodlight这么久了，以前做的最多的是调用RET API进行一些实验，改过官方为开发者提供的MACTracker代码，但实际上没花太多时间看源码，最近做实验啥的感觉瓶颈了，唉，遂决心好好研究一下源码，看是否能给自己新的灵感。此次参考的源码是我修改过的MACTracker，我将要分析的是其中对Packet_in事件处理的方法recevie（）   </p>
<h2 id="Packet_in处理分析">Packet_in处理分析</h2>
<p>首先，贴上代码，主要功能是获取连上控制器的主机的MAC地址，IP地址并在控制台显示出来。   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> net.floodlightcontroller.core.IListener.Command <span class="title">receive</span>(IOFSwitch sw, OFMessage msg, FloodlightContext cntx) {</div><div class="line">Ethernet eth = IFloodlightProviderService.bcStore.get(cntx,IFloodlightProviderService.CONTEXT_PI_PAYLOAD);</div><div class="line">Long sourceMACHash = Ethernet.toLong(eth.getSourceMACAddress());</div><div class="line"><span class="keyword">if</span>(eth.getPayload() <span class="keyword">instanceof</span> IPv4){</div><div class="line">  IPv4 pkt = (IPv4)eth.getPayload().clone();</div><div class="line">  String src = IPv4.fromIPv4Address(pkt.getSourceAddress());</div><div class="line">  String dst = IPv4.fromIPv4Address(pkt.getDestinationAddress());</div><div class="line">  <span class="keyword">if</span> (!macAddresses.contains(sourceMACHash)) {</div><div class="line">	 macAddresses.add(sourceMACHash);</div><div class="line">	 logger.info(<span class="string">"MAC Address: {} IP Address: {} seen on switch"</span>,HexString.toHexString(sourceMACHash),src);</div><div class="line">      }</div><div class="line">  }</div><div class="line"><span class="keyword">return</span> Command.CONTINUE;</div><div class="line">}</div></pre></td></tr></table></figure>


<h3 id="Packet_in包处理流程">Packet_in包处理流程</h3>
<p>1.receive()是模块用来处理Packt_in数据包的函数，每个需要处理Packet_in包的模块均默认以它命名，所以它们获取OpenFlow消息的方式也是相同的。<br>2.首先通过代码：   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Ethernet eth = IFloodlightProviderService.bcStore.get(cntx, IFloodlightProviderService.CONTEXT_PI_PAYLOAD)</div></pre></td></tr></table></figure>


<p>获取消息。其中通过调用接口IFloodlightProviderService的静态方法bcstore获取OpenFlow消息，而该消息是一个Ethernet对象。我们可以通过Ethernet里的方法getSourceMACaddress()获取该报文的源MAC地址；然后再通过getPayload()获取报文的有效载荷(所谓有效载荷，指的是除去协议头部之外实际传输的数据)，根据有效载荷可以判断报文的协议类型：ARP,IPv4,ICMP,DUCP等网络层协议类型。<br>3.若是IPv4包：IPv4类里有个方法getSourceAddress()获取源IP地址(二进制)，通过方法fromIPv4Address将IP地址转换为点分十进制的。   </p>
<h3 id="bcstore">bcstore</h3>
<p>bcstore是接口IFloodlightProvider里定义的一个对象，主要是存储OpenFlow消息，消息的对象是Ethernet类型的。bcstore本身是FloodightContextStore&lt;&gt;类型的，该类型有个方法用来获取存储的消息：get(FloodlightContext bc,String key).方法具体实现如下：   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> V <span class="title">get</span>(FloodlightContext bc, String key) {</div><div class="line">        <span class="keyword">return</span> (V)bc.storage.get(key);}</div></pre></td></tr></table></figure>


<h3 id="return-Command-CONTINUE">return.Command.CONTINUE</h3>
<p>最开始照着开发者文档做的时候，看到recieve()结尾处的return.Command.CONTINUE的解释是“该方法返回Command.CONTINUE以便其他处理程序继续处理”，而且我查看了其他好几个模块recieve函数返回值都是Command.CONTINUE，于是就找到了Command的源头。原来Command是接口<em>Ilistener</em>下的一个枚举类型：   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">enum</span> Command {</div><div class="line">        CONTINUE, STOP</div><div class="line">    }</div></pre></td></tr></table></figure>


<p>当时我就纳闷了，返回一个枚举类型值，谁来处理它？怎么处理这个值以实现能让其他模块继续执行的功能？于是我挨个模块源码寻找,终于在模块<code>net.floodlight.core.internal.controller</code>找到了它的身影：   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">pktinProcTime.recordStartTimePktIn();</div><div class="line">Command cmd;</div><div class="line"><span class="keyword">for</span> (IOFMessageListener listener : listeners) {</div><div class="line">     pktinProcTime.recordStartTimeComp(listener);</div><div class="line">     cmd = listener.receive(sw, m, bc);</div><div class="line">     pktinProcTime.recordEndTimeComp(listener);</div><div class="line">     <span class="keyword">if</span> (Command.STOP.equals(cmd)) {</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">     }</div><div class="line">}</div></pre></td></tr></table></figure>


<p>这段代码就是对recevie()返回值进行处理，如果返回的是STOP，则该事件停止处理，也就是当前模块处理完后其他模块就没法处理了，而默认情况下事件会被每个模块遍历处理，所以我们会看到大多处理事件的模块返回的值都是CONTINUE。   </p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2>
<p>看源码最痛苦的是知其然而不知其所以然，当然也有人建议我不要在意这些细节~~，可是看不懂浑身不舒服啊亲，我也希望自己有高超的写代码能力，事实是还需努力。接触Floodlight这么久了，以前做的最多的是调用RET API进行一些实验，改]]>
    </summary>
    
      <category term="Floodlight" scheme="http://liushy.com/tags/Floodlight/"/>
    
      <category term="Packet_in" scheme="http://liushy.com/tags/Packet-in/"/>
    
      <category term="recevie" scheme="http://liushy.com/tags/recevie/"/>
    
      <category term="SDN" scheme="http://liushy.com/categories/SDN/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于floodlight&sflow的队列调整]]></title>
    <link href="http://liushy.com/2015/02/02/sflow-queue/"/>
    <id>http://liushy.com/2015/02/02/sflow-queue/</id>
    <published>2015-02-02T12:16:13.000Z</published>
    <updated>2016-11-26T10:48:10.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>介绍本次实验前，我想先讲讲关于流量监控的事。嗯，<a href="http://liushy.com/2015/01/27/sflow-ddos/" target="_blank" rel="external">基于sflow的ddos防御</a> 的成功，坚定了我继续基于sflow做流量监控。对于流量监控，我最初的想法是通过openflow协议的count和meter入手。不幸的是，通过count能统计端口数据包和字节数，难以实现速率的监控；meter可以对端口进行速率控制，但<a href="https://github.com/openvswitch/ovs/blob/master/FAQ.md#q-does-open-vswitch-support-openflow-meters" target="_blank" rel="external">OVS FAQ</a>有提到OVS2.0以后都支持了meter,可是还没有实现meter的功能。回到主题，这一次的实验是基于sflow监控的基础上，通过floodlight进行的队列调整。   </p>
<h2 id="实验设计">实验设计</h2>
<p>1.拓扑结构沿用上次实验的：switch+3hosts+floodlight+sflow如图：<br>　　<img src="/img/ddos-topo.png" alt="queue-topo"><br>2.设计思想：switch上有三个端口分别连接有host1,host2,host3。在端口3上设置3条队列，分别带宽设置为：<br>— —id=@q0 create queue other-config:min-rate=1000000000 other-config:max-rate=1000000000<br>— —id=@q1 create queue other-config:max-rate=20000000 other-config:min-rate=20000000<br>— —id=@q2 create queue other-config:max-rate=2000000 other-config:min-rate=2000000<br>让host1和host2同时向host3发包通过流量监控，获取端口1和端口2的速率分别为R1和R2，然后进行判断：<br>　　若R1&gt;R2:端口1到端口3的队列—&gt;q0    端口2到端口3的队列—&gt;q2<br>　　若R2&gt;R1:端口1到端口3的队列—&gt;q2    端口2到端口3的队列—&gt;q0<br>　　若R1=R2:端口1到端口3的队列—&gt;q1    端口2到端口3的队列—&gt;q1    </p>
<h2 id="结果分析">结果分析</h2>
<p>首先让host1和host2分别向host3泛洪：<br>　　h1&gt; ping -f h3<br>　　h2&gt; ping -f h3<br>执行队列调整应用后终端显示:<br>　　<img src="/img/console-view.png" alt="console"><br>可以看出，由于端口1(注意：eth0~eth2依次对应端口1~3)的速率比端口2的速率快，应用立即通知控制器下发流表给端口1和端口2设定不同的出口队列，再来看看控制器的流表：<br>　　<img src="/img/queue-flows.png" alt="queue-flow"><br>从流表可以看到，控制器已经下发了队列调整流表项。端口1的速率变化：<br>　　<img src="/img/s1-eth0-rate.png" alt="eth0-rate"><br>端口2的速率变化：<br>　　<img src="/img/s1-eth1-rate.png" alt="eth1-rate"><br>从图中我们可以看到，最初阶段，端口1和端口2的速率均在2M左右；从时间17:13:18开始，也就是实施队列调整应用后，端口1的速率上升到了大于2M阶段，而端口2的速率下降到了500k以下。</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2>
<p>介绍本次实验前，我想先讲讲关于流量监控的事。嗯，<a href="http://liushy.com/2015/01/27/sflow-ddos/" target="_blank" rel="external">基于sflow的ddo]]>
    </summary>
    
      <category term="sflow" scheme="http://liushy.com/tags/sflow/"/>
    
      <category term="floodlight" scheme="http://liushy.com/tags/floodlight/"/>
    
      <category term="queue" scheme="http://liushy.com/tags/queue/"/>
    
      <category term="qos" scheme="http://liushy.com/tags/qos/"/>
    
      <category term="SDN" scheme="http://liushy.com/categories/SDN/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[sflow流量监控之ddos防御]]></title>
    <link href="http://liushy.com/2015/01/27/sflow-ddos/"/>
    <id>http://liushy.com/2015/01/27/sflow-ddos/</id>
    <published>2015-01-27T08:17:47.000Z</published>
    <updated>2016-12-01T14:06:35.000Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>最近做的一个实验，需要获取链路接口的实时信息，比如带宽，流量统计等等。起初，我打算从openflow协议中的计数器入手，openflow交换机对每一个流维护一个计数器，控制器可以从这些计数器上查询每条链路的实时流量信息。随着网络规模增大，流量增加时，对计数器管理会变得越来越消耗系统资源，如<a href="http://www.openflowhub.org/display/floodlightcontroller/FAQ+Floodlight+OpenFlow+Controller" title="Floodlight FAQ" target="_blank" rel="external">Floodlight FAQ</a>所提到对控制器而言这样的监控很难准确的，所以就否定了在控制器上实现流量监控的想法，转而考虑通过第三方平台监控每条链路的实时流量信息。sflow可以提供周期性的网络接口统计采样和数据包采样，能够提供各接口的流量信息，且几乎不会对被统计设备造成任何负担，管理成本极低。sflow的部署分为两部分：sflow agent和sflow collector。sflow agent内嵌入网络设备中获取设备的实时信息并封装成sflow报文发送给sflow collector。sflow collector汇总后得出统计数据。初次使用sflow监控流量，做了一个ddos防御的实验。   </p>
<h2 id="实验环境">实验环境</h2>
<p>本次实验是在一台物理主机上完成实验拓扑，控制器部署和sflow部署。通过mininet模拟一个switch，三台host。控制器使用Floodlight。由于mininet已经部署了sflow agent，所以只需要部署sflow collector。<br>实验拓扑如下图：<br><img src="/img/ddos-topo.png" alt="ddos-topo"><br>sflow官网推荐了几款sflow软件如sflow-trend,sflow-rt等，这里我选择的是sflow-rt，安装sflow-rt很简单。<br>1.首先，下载<a href="http://www.inmon.com/products/sFlow-RT.php" target="_blank" rel="external">官方压缩包</a><br>2.然后解压安装：    </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$tar</span> -zxvf sflow.tar.gz   </div><div class="line"><span class="variable">$cd</span> sflow/sflow-rt   </div><div class="line">$./start.sh</div></pre></td></tr></table></figure>


<p>在ovs交换机上还要配置sflow agent，输入以下命令：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$sudo</span> ovs-vsctl -- --id=@sflow create sflow agent=eth0 target=\<span class="string">"192.168.2.233:6343\" header=128 sampling=10 polling=1 -- set bridge s1 sflow=@sflow</span></div></pre></td></tr></table></figure>


<p>注意：agent是要监听的网卡，这个网卡一定要能监听到我们所需的交换机的流量，target是sflow collector所在的ip地址，bridge设定需要监听的交换机。   </p>
<h2 id="实验原理">实验原理</h2>
<p>sflow-rt统计到的每个接口的流量信息，我们可以通过sflow-rt的rest api获取json数据并对json数据进行解析获得。对解析到的数据进行判断分析后即可实施策略。本次实验原理如下：<br>1.首先对sflow-rt进行配置，设定metric=ddos,并设定它的阈值，当监测到的流量超过这个阈值时即判断为ddos；<br><em>定义地址组</em>：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -H <span class="string">"Content-Type:application/json"</span> -X PUT --data <span class="string">"{external:['0.0.0.0/0'], internal:['10.0.0.0/8']}"</span> http://localhost:<span class="number">8008</span>/group/json</div></pre></td></tr></table></figure>


<p><em>定义流 </em>：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -H <span class="string">"Content-Type:application/json"</span> -X PUT --data <span class="string">"{keys:'ipsource,ipdestination', value:'frames', filter:'sourcegroup=external&destinationgroup=internal'}"</span> http://localhost:<span class="number">8008</span>/flow/incoming/json</div></pre></td></tr></table></figure>


<p><em>定义阈值</em>：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -H <span class="string">"Content-Type:application/json"</span> -X PUT --data <span class="string">"{metric:'ddos', value:1000}"</span> http://localhost:<span class="number">8008</span>/threshold/incoming/json</div></pre></td></tr></table></figure>

<p>2.若判断为ddos，即调用Floodlight的staticflowentrypusher对ddos攻击包进行丢弃；<br>由于sflow获取的的openflow信息是使用snmp中定义的ifindex对各接口进行标记，而openflow有它自己的标记方式，所以应该对openflow端口号和ifindex端口号进行映射。本次实验采用nodejs作为应用语言。    </p>
<h2 id="实验结果">实验结果</h2>
<p>本次实验ddos攻击采取host1向host2泛洪的方式。命令如下：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mininet&gt;h1 ping <span class="operator">-f</span> h2</div></pre></td></tr></table></figure>


<p>运行ddos防御应用前：<br><img src="/img/ddos泛洪.png" alt="ddos"><br>我们可以看到，未运行ddos防御应用时，h1向h2泛洪的数据包达到了大约每秒30k个包。<br>接下来运行ddos防御应用：   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">nodejs ddosm.js</div></pre></td></tr></table></figure>


<p>运行ddos防御应用后：<br><img src="/img/ddos截取.png" alt="ddos2"><br>可以看出，运行ddos防御应用后，h1向h2泛洪的包迅速被完全的丢弃了。   </p>
<h2 id="总结">总结</h2>
<p>写这篇文主要目的不是介绍怎么写应用，而是对sflow性能的一个展示。流量监控是sdn中很重要的一环，我们在获取各个接口的实时信息后，可以实现很多的服务，比如负载均衡，QOS，流量工程等。这也是我初次尝试sflow,还有很多不解的地方，我的想法是，做好sflow与控制器的交互，完善流量监控的功能，为以后的各种服务提供帮助。<br><strong>参考网站</strong>：<a href="http://blog.sflow.com/2013/05/controlling-large-flows-with-openflow.html" target="_blank" rel="external">sflow官博</a><br><strong>DDOS源码参考</strong>：<a href="https://github.com/shylou/floodlight-ddos" target="_blank" rel="external">DDOS源码</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2>
<p>最近做的一个实验，需要获取链路接口的实时信息，比如带宽，流量统计等等。起初，我打算从openflow协议中的计数器入手，openflow交换机对每一个流维护一个计数器，控制器可以从这些计数器上查询每条链路的实时流量信息。随着网络规模增]]>
    </summary>
    
      <category term="sflow" scheme="http://liushy.com/tags/sflow/"/>
    
      <category term="floodlight" scheme="http://liushy.com/tags/floodlight/"/>
    
      <category term="ddos" scheme="http://liushy.com/tags/ddos/"/>
    
      <category term="SDN" scheme="http://liushy.com/categories/SDN/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[SDN环境下两主机通信过程]]></title>
    <link href="http://liushy.com/2014/11/15/floodlight-twohosts-ping/"/>
    <id>http://liushy.com/2014/11/15/floodlight-twohosts-ping/</id>
    <published>2014-11-15T11:47:25.000Z</published>
    <updated>2016-11-26T10:45:30.000Z</updated>
    <content type="html"><![CDATA[<h2 id="感悟">感悟</h2>
<p>SDN看了也有两个月了吧，刚接触时很兴奋呐，感觉就是一控制器加一交换机呗。弄懂里面的函数结构，接下来搞编程应该就可以了吧~~可是这仅仅是我天真的想法。当别人问我为什么是这样的时候，放到最初那会儿，我会干脆的说：SDN就是这样啊；而现在，我会很小心地分析它的协议流程，害怕漏掉或者说错某个细节。越了解越敬畏，一个概念的产生并且盛行是经过许多学者推敲和认证的，所   以无论是做科研还是学术，我们都应有一个严谨的态度。</p>
<h2 id="两主机通信过程">两主机通信过程</h2>
<p>本次实验环境Floodlight+Mininet。我们利用mininet仿真出一个switch下三台hosts：h1,h2,    h3,然后h2 ping h3;通过wirshark抓包分析openflow的协议流程。（在本文我就不分析不同   switch下不同host的ping包过程了）默认加载了转发模块。拓扑图如下：<br><img src="/img/topo.png" alt="topo">     </p>
<p>1.最初h2会通过将自己和h3的ip地址同子网掩码与运算得知：自己和h3在同一网段，可直接通信；<br>2.h2对数据包二层封装时，发现自己并不知道h3的mac地址，于是发送ARP广播包；<br>3.switch收到arp广播包后，由于没有流表，于是它向控制器发送packet_in消息：<br>4.控制器收到packet_in后，向switch发送packet_out,并下发流表给switch让它将数据包从除2端口以外的其他所有端口发送；<br>5.h3收到arp数据包后，在数据包里添加上自己的mac地址；<br>6.switch收到h3的arp包，由于没有流表项，于是向控制器发送packet_in消息；<br>7.控制器学习到h3的mac和ip地址，向switch发送packet_out消息并下发h3到h2的流表项；<br>8.h2知道了h3的mac地址，完成icmp包的封装，就向h3发包了；<br>9.由于switch没有h2-&gt;h3的流表项，所以它还是会向控制器发送packet_in；<br>10.控制器发送packet_out给switch并下发h2-&gt;h3的流表；至此h2和h3就能不通过控制器只通过switch直接通信啦！<br>下面是寻址阶段抓包截图：（注：抓取的是通过eth0的包，eth0与控制器通信，所以也就是抓的控制器的包）<br><img src="/img/ping过程.png" alt="ping过程">     </p>
<p>寻址结束后，抓取控制器的包时，没有了h2 ping h3的icmp包,在端口2抓包：<br><img src="/img/s1-eth2.png" alt="端口2">   </p>
<p>在端口3抓包：<br><img src="/img/s1-eth3.png" alt="端口3">   </p>
<h2 id="Openflow与传统路由的比较">Openflow与传统路由的比较</h2>
<p>Openflow与传统路由一样，在源主机不知道目的主机mac地址的情况下，都会进行arp寻址。但不同之处在于，传统路由没有控制器，所有流表在switch里,转发和控制都由switch负责；而Openflow协议里，switch若没有匹配流表（或者没有流表），它会向控制器发送请求，让控制器下发策略。<br><strong>这是我个人的分析，欢迎指正，大家共同学习，若转载请注明出处！</strong></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="感悟">感悟</h2>
<p>SDN看了也有两个月了吧，刚接触时很兴奋呐，感觉就是一控制器加一交换机呗。弄懂里面的函数结构，接下来搞编程应该就可以了吧~~可是这仅仅是我天真的想法。当别人问我为什么是这样的时候，放到最初那会儿，我会干脆的说：SDN就是这样啊；而现在]]>
    </summary>
    
      <category term="floodlight" scheme="http://liushy.com/tags/floodlight/"/>
    
      <category term="ping" scheme="http://liushy.com/tags/ping/"/>
    
      <category term="openflow" scheme="http://liushy.com/tags/openflow/"/>
    
      <category term="SDN" scheme="http://liushy.com/categories/SDN/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[openwrt-example]]></title>
    <link href="http://liushy.com/2014/11/14/openwrt-example/"/>
    <id>http://liushy.com/2014/11/14/openwrt-example/</id>
    <published>2014-11-14T15:27:00.000Z</published>
    <updated>2016-11-27T02:03:56.000Z</updated>
    <content type="html"><![CDATA[<p>这里的开发环境是在Ubuntu下，搭建好OpenWrt的交叉编译环境，这里不多说了，网上有很多教程。本次示例选择的OpenWrt版本是trunk版.     </p>
<a id="more"></a>     

<p>接下来是添加模块具体步骤：<br>1.进入trunk的package文件夹，创建模块目录：<br>cd trunk/package<br>mkdir example<br>2.进入example目录，创建Mackefile文件和代码路径：<br>cd example<br>touch Mackefile<br>mkdir src<br>Makefile代码如下：  </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">include</span> <span class="variable">$(</span><span class="constant">TOPDIR</span>)/rules.mk   </div><div class="line"><span class="keyword">include</span> <span class="variable">$(</span><span class="constant">INCLUDE_DIR</span>)/kernel.mk   </div><div class="line"><span class="constant">PKG_NAME</span><span class="symbol">:</span>=example</div><div class="line"><span class="constant">PKG_RELEASE</span><span class="symbol">:</span>=<span class="number">1</span></div><div class="line"></div><div class="line"><span class="keyword">include</span> <span class="variable">$(</span><span class="constant">INCLUDE_DIR</span>)/package.mk</div><div class="line"></div><div class="line">define <span class="constant">KernelPackage</span>/example</div><div class="line">  <span class="constant">SUBMENU</span><span class="symbol">:</span>=<span class="constant">Other</span> modules</div><div class="line">  <span class="constant">TITLE</span><span class="symbol">:</span>=example driver</div><div class="line">  <span class="constant">DEPENDS</span><span class="symbol">:</span>=<span class="variable">@LINUX_2_6</span></div><div class="line">  <span class="constant">FILES</span><span class="symbol">:</span>=<span class="variable">$(</span><span class="constant">PKG_BUILD_DIR</span>)/*.<span class="variable">$(</span><span class="constant">LINUX_KMOD_SUFFIX</span>)</div><div class="line">  <span class="constant">KCONFIG</span><span class="symbol">:</span>=</div><div class="line">endef</div><div class="line"></div><div class="line">define <span class="constant">KernelPackage</span>/example/description</div><div class="line">  <span class="constant">Kernel</span> <span class="class"><span class="keyword">module</span> <span class="title">to</span> <span class="title">example</span></span></div><div class="line">endef</div><div class="line"></div><div class="line"><span class="constant">EXTRA_KCONFIG</span><span class="symbol">:</span>= \</div><div class="line">	<span class="constant">CONFIG_EXAMPLE</span>=m</div><div class="line"></div><div class="line"><span class="constant">EXTRA_CFLAGS</span><span class="symbol">:</span>= \</div><div class="line">	<span class="variable">$(</span>patsubst <span class="constant">CONFIG_</span>%, -<span class="constant">DCONFIG_</span>%=<span class="number">1</span>, <span class="variable">$(</span>patsubst %=m,%,<span class="variable">$(</span>filter %=m,<span class="variable">$(</span><span class="constant">EXTRA_KCONFIG</span>)))) \</div><div class="line">	<span class="variable">$(</span>patsubst <span class="constant">CONFIG_</span>%, -<span class="constant">DCONFIG_</span>%=<span class="number">1</span>, <span class="variable">$(</span>patsubst %=y,%,<span class="variable">$(</span>filter %=y,<span class="variable">$(</span><span class="constant">EXTRA_KCONFIG</span>)))) \</div><div class="line"></div><div class="line"><span class="constant">MAKE_OPTS</span><span class="symbol">:</span>= \</div><div class="line">	<span class="constant">ARCH</span>=<span class="string">"$(LINUX_KARCH)"</span> \</div><div class="line">	<span class="constant">CROSS_COMPILE</span>=<span class="string">"$(TARGET_CROSS)"</span> \</div><div class="line">	<span class="constant">SUBDIRS</span>=<span class="string">"$(PKG_BUILD_DIR)"</span> \</div><div class="line">	<span class="constant">EXTRA_CFLAGS</span>=<span class="string">"$(EXTRA_CFLAGS)"</span> \</div><div class="line">	<span class="variable">$(</span><span class="constant">EXTRA_KCONFIG</span>)</div><div class="line"></div><div class="line">define <span class="constant">Build</span>/<span class="constant">Prepare</span></div><div class="line">	mkdir -p <span class="variable">$(</span><span class="constant">PKG_BUILD_DIR</span>)</div><div class="line">	<span class="variable">$(</span><span class="constant">CP</span>) ./src/* <span class="variable">$(</span><span class="constant">PKG_BUILD_DIR</span>)/</div><div class="line">endef</div><div class="line"></div><div class="line">define <span class="constant">Build</span>/<span class="constant">Compile</span></div><div class="line">	<span class="variable">$(</span><span class="constant">MAKE</span>) -<span class="constant">C</span> <span class="string">"$(LINUX_DIR)"</span> \</div><div class="line">		<span class="variable">$(</span><span class="constant">MAKE_OPTS</span>) \</div><div class="line">		modules</div><div class="line">endef</div><div class="line"></div><div class="line"><span class="variable">$(</span>eval <span class="variable">$(</span>call <span class="constant">KernelPackage</span>,example))</div></pre></td></tr></table></figure>



<p>3.进入src目录，创建代码路径和相关源文件<br>cd src<br>touch example.c Kconfig Makefile<br>example.c代码：    </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor">#<span class="keyword">include</span> &lt;linux/init.h&gt;</span></div><div class="line"><span class="preprocessor">#<span class="keyword">include</span> &lt;linux/module.h&gt;</span></div><div class="line"><span class="preprocessor">#<span class="keyword">include</span> &lt;linux/kernel.h&gt;</span></div><div class="line"><span class="comment">/* hello_init ---- 初始化函数，当模块装载时被调用，如果成功装载返回0 否则返回非0值 */</span></div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> __init hello_init(<span class="keyword">void</span>)</div><div class="line">{</div><div class="line">	printk(<span class="string">"I bear a charmed life.\n"</span>);</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">}</div><div class="line"><span class="comment">/* hello_exit ---- 退出函数，当模块卸载时被调用 */</span></div><div class="line"><span class="keyword">static</span> <span class="keyword">void</span> __exit hello_exit(<span class="keyword">void</span>)</div><div class="line">{</div><div class="line">	printk(<span class="string">"Out, out, brief candle\n"</span>);</div><div class="line">}</div><div class="line">module_init(hello_init);</div><div class="line">module_exit(hello_exit);</div><div class="line">MODULE_LICENSE(<span class="string">"GPL"</span>);</div><div class="line">MODULE_AUTHOR(<span class="string">"liuxie"</span>);</div></pre></td></tr></table></figure>







]]></content>
    <summary type="html">
    <![CDATA[<p>这里的开发环境是在Ubuntu下，搭建好OpenWrt的交叉编译环境，这里不多说了，网上有很多教程。本次示例选择的OpenWrt版本是trunk版.     </p>
]]>
    
    </summary>
    
      <category term="OpenWrt" scheme="http://liushy.com/tags/OpenWrt/"/>
    
      <category term="Linux" scheme="http://liushy.com/categories/Linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hello World]]></title>
    <link href="http://liushy.com/2014/11/14/hello-world/"/>
    <id>http://liushy.com/2014/11/14/hello-world/</id>
    <published>2014-11-14T15:23:15.000Z</published>
    <updated>2014-11-14T08:07:18.000Z</updated>
    <content type="html"><![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">trobuleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2>
<h3 id="Create_a_new_post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>

<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>

<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>

<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>

<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io]]>
    </summary>
    
  </entry>
  
</feed>
