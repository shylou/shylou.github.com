<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>使用docker部署hadoop分布式集群 | liushy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="前言这个部署是去年做的，有点久了，镜像上传在dockerhub，想了想，还是写下来，万一哪天有用，可以回顾一下。关于docker的一些内容，后序会写一些，碍于目前生产环境的限制，能做的不会太多，但更多的是熟练docker。hadoop呢，以前读书的时候，断断续续地研究过一段时间，然后就没然后了，事情太多太杂，而自身精力涣散，三天打鱼两天晒网，实际也就没弄明白什么。平时读点博客，看到docker，h">
<meta property="og:type" content="article">
<meta property="og:title" content="使用docker部署hadoop分布式集群">
<meta property="og:url" content="https://liushy.com/2016/12/21/docker+hadoop/index.html">
<meta property="og:site_name" content="liushy">
<meta property="og:description" content="前言这个部署是去年做的，有点久了，镜像上传在dockerhub，想了想，还是写下来，万一哪天有用，可以回顾一下。关于docker的一些内容，后序会写一些，碍于目前生产环境的限制，能做的不会太多，但更多的是熟练docker。hadoop呢，以前读书的时候，断断续续地研究过一段时间，然后就没然后了，事情太多太杂，而自身精力涣散，三天打鱼两天晒网，实际也就没弄明白什么。平时读点博客，看到docker，h">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://liushy.com/imgs/docker-hadoop.png">
<meta property="article:published_time" content="2016-12-20T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-28T15:22:36.370Z">
<meta property="article:author" content="liushy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liushy.com/imgs/docker-hadoop.png">
  
    <link rel="alternate" href="/atom.xml" title="liushy" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">liushy</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/about/">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://liushy.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-docker+hadoop" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/12/21/docker+hadoop/" class="article-date">
  <time class="dt-published" datetime="2016-12-20T16:00:00.000Z" itemprop="datePublished">2016-12-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/linux/">linux</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      使用docker部署hadoop分布式集群
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2><span id="前言">前言</span></h2><p>这个部署是去年做的，有点久了，镜像上传在<a target="_blank" rel="noopener" href="https://hub.docker.com/r/liushy/ubuntu/tags/">dockerhub</a>，想了想，还是写下来，万一哪天有用，可以回顾一下。关于docker的一些内容，后序会写一些，碍于目前生产环境的限制，能做的不会太多，但更多的是熟练docker。hadoop呢，以前读书的时候，断断续续地研究过一段时间，然后就没然后了，事情太多太杂，而自身精力涣散，三天打鱼两天晒网，实际也就没弄明白什么。平时读点博客，看到docker，hadoop等字眼就会不住高潮，内心想说这玩意我玩过啊，然后也就仅限于此了。术业有专攻，如果不是经常接触，搞不久就会忘，若时常回顾一下，等到用时，就不会那么陌生了。<br>  <span id="more"></span>    </p>
<h2><span id="安装docker">安装docker</span></h2><p>安装docker的套路网上有教程，目前新的Ubuntu系统都有集成，不过<a target="_blank" rel="noopener" href="https://www.oschina.net/translate/installing-docker-on-mac-os-x">OSX系统跑docker</a>需要配合虚拟机如Virtualbox等来做，所以mac用户要稍微折腾一下了。本文的环境是在Ubuntu 14下做的，大致命令如下：  </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install apt-transport-https </span><br><span class="line">$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9 </span><br><span class="line">$ sudo bash -c <span class="string">&quot;echo deb https://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list&quot;</span> </span><br><span class="line">$ sudo apt-get update </span><br><span class="line">$ sudo apt-get install lxc-docker</span><br></pre></td></tr></table></figure>
<p>docker的安装可以参考<a target="_blank" rel="noopener" href="http://www.cnblogs.com/xiaoluosun/p/5520510.html">这个</a><br>接下来要提到的是docker仓库的概念，熟悉github或使用过svn的都应该知道代码库吧，而我们也可以为docker创建仓库，这个库即是存放docker镜像的场所。比如我们做了个装有JDK的docker镜像，那么就可以将其存到仓库里，每次需要使用带Java的环境时，就可以可以使用该镜像，并在此基础上构建其它镜像。国内比较知名的docker库如<a target="_blank" rel="noopener" href="https://github.com/DockerPool/">dockerpool</a>，而我此次使用的是dockerhub，国内访问剧慢，最好使用vpn加速。建议养成使用仓库的习惯，无论是代码还是docker。怎么使用docker仓库呢，首先需要在dockerhub官网创建一个账户，创建自己的仓库，然后在你的生产环境登陆dockerhub，即可从dockerhub上pull（拉取）镜像，当然也可以将本地的镜像push（上传）到仓库。大致流程可<a target="_blank" rel="noopener" href="http://geek.csdn.net/news/detail/35121">参考此文</a>。<br>登录到DockerHub：   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker login --username=账户 --email=邮箱@xxx.com</span><br><span class="line">PassWord:</span><br><span class="line">WARNING: login credentials saved <span class="keyword">in</span> /home/hadoop/.docker/config.json</span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure>
<p>搞定仓库设置后，那么就可以从docker仓库中获取Ubuntu镜像了：   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull ubuntu:14.04</span><br></pre></td></tr></table></figure>
<p>使用docker images可以查看本地的所有镜像：    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop@hadoop:~$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE</span><br><span class="line">liushy/ubuntu       hadoop              358d6e31872e        13 months ago       1.276 GB</span><br><span class="line">liushy/ubuntu       java                50d62690f3f0        13 months ago       752.9 MB</span><br><span class="line">ubuntu              14.04               e9ae3c220b23        13 months ago       187.9 MB</span><br></pre></td></tr></table></figure>
<p>显示的内容包括：   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">REPOSITORY：仓库名，例如liushy/ubuntu和ubuntu</span><br><span class="line">TAG：标记,例如hadoop</span><br><span class="line">IMAGE ID：镜像ID号，这是唯一的</span><br><span class="line">CREATED：创建时间</span><br><span class="line">SIZE：镜像大小</span><br></pre></td></tr></table></figure>
<p>那么，除此之外，还需要了解docker的一些常用操作。    </p>
<h2><span id="启动docker容器">启动Docker容器</span></h2><p>启动docker容器，就可以构建部署hadoop集群了。使用如下命令启动docker容器:      </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -ti ubuntu</span><br></pre></td></tr></table></figure>
<p>docker run -ti ubuntu命令中没有指定执行程序，Docker默认执行/bin/bash。如下面这条命令：    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop@hadoop:~$  docker run liushy/ubuntu:java /bin/<span class="built_in">echo</span> <span class="string">&#x27;Hello world&#x27;</span></span><br><span class="line">Hello world</span><br></pre></td></tr></table></figure>
<p>即是启动标记为java的镜像，运行bash打印“Hello world”。<br>接下来我们在Ubuntu基础镜像的上，安装Java，执行如下命令：    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@50d62690f3f0:~<span class="comment">#sudo apt-get install software-properties-common python-software-properties</span></span><br><span class="line">root@50d62690f3f0:~<span class="comment">#sudo add-apt-repository ppa:webupd8team/java</span></span><br><span class="line">root@50d62690f3f0:~<span class="comment">#sudo apt-get update</span></span><br><span class="line">root@50d62690f3f0:~<span class="comment">#sudo apt-get install oracle-java7-installer</span></span><br></pre></td></tr></table></figure>
<p>安装结束后，可以将该镜像保存，以备以后使用：    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@50d62690f3f0:~<span class="comment"># exit</span></span><br><span class="line">docker commit -m <span class="string">&quot;java image&quot;</span> 50d62690f3f0  liushy/ubuntu:java</span><br></pre></td></tr></table></figure>
<p><code>-m</code>后面指定提交说明，<code>50d62690f3f0</code>是容器ID（也可以通过docker ps查询运行的容器），<code>liushy/ubuntu</code>是仓库，<code>:java</code>是标记。  </p>
<h2><span id="构建hadoop镜像">构建Hadoop镜像</span></h2><p>首先启动Java容器的镜像，下载Hadoop，输入如下命令：   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop@hadoop:~$ docker run -ti liushy/ubuntu:java</span><br><span class="line">root@7cf73e2147ec:/<span class="comment"># </span></span><br><span class="line">root@7cf73e2147ec:<span class="built_in">cd</span> ~</span><br><span class="line">root@7cf73e2147ec:~<span class="comment"># mkdir soft</span></span><br><span class="line">root@7cf73e2147ec:~<span class="comment"># cd soft/</span></span><br><span class="line">root@7cf73e2147ec:~/soft<span class="comment"># mkdir apache</span></span><br><span class="line">root@7cf73e2147ec:~/soft<span class="comment"># cd apache/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache<span class="comment"># mkdir hadoop</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache<span class="comment"># cd hadoop/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop<span class="comment"># wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop<span class="comment"># tar xvzf hadoop-2.6.0.tar.gz</span></span><br></pre></td></tr></table></figure>
<p>本次安装的是hadoop-2.6.0版的。</p>
<h3><span id="配置环境">配置环境</span></h3><p>还需要配置环境变量，在~/.bashrc添加java和hadoop文件路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@7cf73e2147ec:vi ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-7-oracle</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/root/soft/apache/hadoop/hadoop-2.6.0</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONFIG_HOME=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>
<p>最后，一定记得要<code>source ~/.bashrc</code>使配置生效。    </p>
<h3><span id="配置hadoop">配置Hadoop</span></h3><p>部署分布式的hadoop，主要分为两大角色：Master和Slave。从HDFS的角度，由若干个NameNode和DataNode组成（在分布式文件系统中，NameNode管理文件系统的命名空间，DataNode管理存储的数据）；从MapReduce的角度，将主机划分JobTracker 和TaskTracker(主节点的Job分配多个Task给从节点执行)。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务<a target="_blank" rel="noopener" href="http://blog.chinaunix.net/uid-25266990-id-3900239.html">参考</a>。</p>
<p>部署hadoop的各个节点，需要修改hadoop的配置文件，包括core-site.xml、hdfs-site.xml、mapred-site.xml这三个文件。在此之前，建议新建如下目录:   </p>
<ul>
<li>tmp：作为Hadoop的临时目录      </li>
<li>namenode：作为NameNode的存放目录     </li>
<li>datanode：作为DataNode的存放目录         <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@7cf73e2147ec:~<span class="comment"># cd $HADOOP_HOME/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># mkdir tmp</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># cd tmp/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0/tmp<span class="comment"># cd ../</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># mkdir namenode</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># cd namenode/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0/namenode<span class="comment"># cd ../</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># mkdir datanode</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0<span class="comment"># cd datanode/</span></span><br><span class="line">root@7cf73e2147ec::~/soft/apache/hadoop/hadoop-2.6.0/datanode<span class="comment"># cd $HADOOP_CONFIG_HOME/</span></span><br><span class="line">root@7cf73e2147ec:~/soft/apache/hadoop/hadoop-2.6.0/etc/hadoop<span class="comment"># cp mapred-site.xml.template mapred-site.xml</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>接下来就是对上述几个文件进行配置      </p>
<p><strong>core-site.xml配置</strong>            </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">description</span>&gt;</span>xxxxxx.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>hadoop.tmp.dir：配置为/root/soft/apache/hadoop/hadoop-2.6.0/tmp为此前创建的临时目录    </li>
<li>fs.default.name：配置为hdfs://master:9000，指向Master节点                  </li>
</ul>
<p><strong>hdfs-site.xml配置</strong>            </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>xxxxx.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/soft/apache/hadoop/hadoop-2.6.0/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">final</span>&gt;</span>true<span class="tag">&lt;/<span class="name">final</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>dfs.replication：配置为2。指集群为一个Master节点和两个Slave节点。     </li>
<li>dfs.namenode.name.dir：配置为此前创建的NameNode目录      </li>
<li>dfs.datanode.data.dir：配置为此前创建的NaDataNode目录    </li>
</ul>
<p><strong>mapred-site.xml配置</strong>      </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>xxxxxx.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>mapred.job.tracker：配置jobTracker在master节点     </li>
</ul>
<p>除此之外，还要配置conf/hadoop-env.sh文件,修改为你的jdk的安装位置:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-7-oracle</span><br></pre></td></tr></table></figure>
<p>还要格式化Namenode:<code>hadoop namenode -format</code>   </p>
<h3><span id="节点ssh互访">节点SSH互访</span></h3><p>Hadoop启动以后，Namenode是通过SSH来启动和停止各个Datanode上的守护进程的，要求在节点之间执行指令的时候是不需要输入密码，故我们要配置SSH运用无密码公钥认证的形式。<br>首先，安装SSH:         </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@7cf73e2147ec:~<span class="comment"># sudo apt-get install ssh</span></span><br><span class="line">root@7cf73e2147ec:~<span class="comment"># ssh localhost</span></span><br></pre></td></tr></table></figure>
<p>利用<code>ssh localhost</code>测试一下是否设置好无口令登陆，如果没有设置好，系统将要求你输入密码，通过下面的设置可以实现无口令登陆:    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@7cf73e2147ec:~<span class="comment"># ssh-keygen -t dsa -P &#x27;&#x27; -f ~/.ssh/id_dsa</span></span><br><span class="line">root@7cf73e2147ec:~<span class="comment"># cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure>
<p>最后，保存一份镜像:    </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@7cf73e2147ec:~<span class="comment"># exit</span></span><br><span class="line">hadoop@hadoop:~$ docker commit -m <span class="string">&quot;hadoop image&quot;</span> 358d6e31872e liushy/ubuntu:hadoop   </span><br></pre></td></tr></table></figure>

<h2><span id="启动hadoop集群">启动Hadoop集群</span></h2><p>接下来就是启动hadoop集群了，本次部署的hadoop方案是，一个Master节点，两个Slave节点：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">______________________________________________    </span><br><span class="line">| Master | 172.17.0.2 | Namenode,jobTracker  |</span><br><span class="line">| Slave1 | 172.17.0.3 | Datanode,taskTracker |</span><br><span class="line">| Slave2 | 172.17.0.4 | Datanode,taskTracker |</span><br></pre></td></tr></table></figure>
<p>启动容器使用如下命令：<br><code>docker run -ti -h master liushy/ubuntu:hadoop</code><br><code>docker run -ti -h slave1 liushy/ubuntu:hadoop</code><br><code>docker run -ti -h slave2 liushy/ubuntu:hadoop</code><br>其中，<code>-h</code>指定的是容器的主机名，比如<code>master</code> <code>slave1</code> <code>slave2</code><br>容器启动成功后，docker会为它们自动分配ip地址，是同一网段相互之间能够ping通，在容器中输入<code>ifconfig</code>查看。当然也可以修改ip，方法请自行百度。   </p>
<h3><span id="修改hosts">修改hosts</span></h3><p>接下来修改各节点的hosts文件<code>vi /etc/hosts</code>，添加各节点的hostname和ip：   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">172.17.0.2        master</span><br><span class="line">172.17.0.3        slave1</span><br><span class="line">172.17.0.4        slave2</span><br></pre></td></tr></table></figure>

<h3><span id="修改slaves">修改slaves</span></h3><p>除此之外，还要在master节点上，配置slaves文件，该文件需要填写slave节点的hostname:   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@master:~<span class="comment"># cd $HADOOP_CONFIG_HOME/</span></span><br><span class="line">root@master:~/soft/apache/hadoop/hadoop-2.6.0/etc/hadoop<span class="comment"># vi slaves</span></span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<h3><span id="启动hadoop">启动Hadoop</span></h3><p>在Master节点的hadoop目录下执行start-all.sh，启动Hadoop集群:   </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@master:~/soft/apache/hadoop/hadoop-2.6.0/etc/hadoop<span class="comment"># start-all.sh </span></span><br><span class="line">This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh</span><br><span class="line">starting secondarynamenode, logging to /root/soft/apache/hadoop/hadoop-2.6.0/logs/hadoop-root-secondarynamenode-master.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">...</span><br><span class="line">0.0.0.0:starting resourcemanager, logging to /root/soft/apache/hadoop/hadoop-2.6.0/logs/yarn--resourcemanager-master.out</span><br><span class="line">slave2: starting nodemanager, logging to /root/soft/apache/hadoop/hadoop-2.6.0/logs/yarn-root-nodemanager-slave2.out</span><br><span class="line">slave1: starting nodemanager, logging to /root/soft/apache/hadoop/hadoop-2.6.0/logs/yarn-root-nodemanager-slave1.out</span><br></pre></td></tr></table></figure>
<p>打印上述日志，说明集群运行成功，碍于生产环境，运行较慢，可以多等等。使用jps命令可以查看各节点运行的进程。   </p>
<p><strong>master节点：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@master:~/soft/apache/hadoop/hadoop-2.6.0/etc/hadoop<span class="comment"># jps</span></span><br><span class="line">492 Jps</span><br><span class="line">429 ResourceManager</span><br><span class="line">295 SecondaryNameNode</span><br><span class="line">124 NameNode</span><br></pre></td></tr></table></figure>

<p><strong>slave1节点：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@slave1:/<span class="comment"># jps</span></span><br><span class="line">52 DataNode</span><br><span class="line">147 NodeManager</span><br><span class="line">240 Jps</span><br></pre></td></tr></table></figure>

<p><strong>slave2节点：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@slave2:/<span class="comment"># jps</span></span><br><span class="line">50 DataNode</span><br><span class="line">121 NodeManager</span><br><span class="line">250 Jps</span><br></pre></td></tr></table></figure>

<p>通过web访问hadoop运行情况：   </p>
<p><img src="/imgs/docker-hadoop.png" alt="hadoop"><br>针对hadoop的架构和操作此文就不深入阐述了。   </p>
<h2><span id="结语">结语</span></h2><p>搭建环境是一件费时耗力，低效率的事情，搞不好就各种排错，甚至重来，这也是很多人不想接触环境的原因。如何做到少出错，快速定位问题，我想这也是很多正在搞环境的开发人员所渴望的一种工作状态。其实，没有什么捷径可走，还是要多积累，但有些事情必须得长期坚持，那就是学习操作系统，内存管理，网络等相关知识。很多疑难杂症多因环境变量的配置，硬盘故障，网络不通等引起，而我们往往因为对这些知识不熟悉，而手足无措。   </p>
<p>回到本文的主题，docker是这两年势头正旺的开源产品，基于lxc（linux container）使得应用部署更加轻量。如何将其运用于开发环境中，很多互联网公司都在做。作为开发人员来说，掌握容器也是一门专业利器，基于容器结合应用可以实现大规模的业务部署，比如本文的hadoop。当然相比虚拟机，docker也有缺陷，比如隔离性差，所以如何运用docker来保障处理业务的安全稳定，是开发人员需要通过实践来解决的问题。  </p>
<p><strong>参考网站</strong><br><a target="_blank" rel="noopener" href="http://blog.chinaunix.net/uid-25266990-id-3900239.html">http://blog.chinaunix.net/uid-25266990-id-3900239.html</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/u011692203/article/details/46898293">http://blog.csdn.net/u011692203/article/details/46898293</a><br><a target="_blank" rel="noopener" href="http://www.cnblogs.com/xiaoluosun/p/5520510.html">http://www.cnblogs.com/xiaoluosun/p/5520510.html</a><br><a target="_blank" rel="noopener" href="http://tashan10.com/yong-dockerda-jian-hadoopwei-fen-bu-shi-ji-qun/">http://tashan10.com/yong-dockerda-jian-hadoopwei-fen-bu-shi-ji-qun/</a>    </p>
<p><strong>个人分析，欢迎指正，若转载请注明出处！</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://liushy.com/2016/12/21/docker+hadoop/" data-id="clexlgj1i00048myyfjrb800d" data-title="使用docker部署hadoop分布式集群" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/12/24/goon-2016/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          贰零幺陆
        
      </div>
    </a>
  
  
    <a href="/2016/11/27/Linux-mem/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Linux内存占用</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/code/">code</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/life/">life</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/network/">network</a><span class="category-list-count">23</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/06/ovn-sec2/">ovn lsp里addresses和port_security的区别</a>
          </li>
        
          <li>
            <a href="/2023/02/18/ovs-del-flows/">ovs删除dp流表后的流量不中断</a>
          </li>
        
          <li>
            <a href="/2022/12/30/goon-2022/">隐入尘烟</a>
          </li>
        
          <li>
            <a href="/2022/06/26/ovn-vlan-issue2/">ovn vlan南北向流量问题</a>
          </li>
        
          <li>
            <a href="/2022/06/25/ovn-vlan-issue1/">ovn vlan东西向流量问题</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 liushy<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/about/" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>